{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Project 2: Supervised Learning\n",
    "### Building a Student Intervention System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Classification vs Regression\n",
    "\n",
    "Your goal is to identify students who might need early intervention - which type of supervised machine learning problem is this, classification or regression? Why?\n",
    "\n",
    "As we want to predict weather a given student will pass or fail, given information about his life and habits, this\n",
    "indicates a classification problem with two classes, pass and fail. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Student data read successfully!\n"
     ]
    }
   ],
   "source": [
    "# Read student data\n",
    "student_data = pd.read_csv(\"student-data.csv\")\n",
    "print \"Student data read successfully!\"\n",
    "# Note: The last column 'passed' is the target/label, all other are feature columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, can you find out the following facts about the dataset?\n",
    "- Total number of students\n",
    "- Number of students who passed\n",
    "- Number of students who failed\n",
    "- Graduation rate of the class (%)\n",
    "- Number of features\n",
    "\n",
    "_Use the code block below to compute these values. Instructions/steps are marked using **TODO**s._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of students: 395\n",
      "Number of students who passed: 265\n",
      "Number of students who failed: 130\n",
      "Number of features: 30\n",
      "Graduation rate of the class: 67.09%\n"
     ]
    }
   ],
   "source": [
    "# TODO: Compute desired values - replace each '?' with an appropriate expression/function call\n",
    "\n",
    "n_students = student_data.shape[0]\n",
    "n_features = student_data.shape[1] - 1\n",
    "n_passed = sum([1 for y in student_data['passed'] if y == 'yes'])\n",
    "n_failed = sum([1 for n in student_data['passed'] if n == 'no'])\n",
    "grad_rate = 100.*n_passed/(n_passed + n_failed)\n",
    "\n",
    "print \"Total number of students: {}\".format(n_students)\n",
    "print \"Number of students who passed: {}\".format(n_passed)\n",
    "print \"Number of students who failed: {}\".format(n_failed)\n",
    "print \"Number of features: {}\".format(n_features)\n",
    "print \"Graduation rate of the class: {:.2f}%\".format(grad_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preparing the Data\n",
    "In this section, we will prepare the data for modeling, training and testing.\n",
    "\n",
    "### Identify feature and target columns\n",
    "It is often the case that the data you obtain contains non-numeric features. This can be a problem, as most machine learning algorithms expect numeric data to perform computations with.\n",
    "\n",
    "Let's first separate our data into feature and target columns, and see if any features are non-numeric.<br/>\n",
    "**Note**: For this dataset, the last column (`'passed'`) is the target or label we are trying to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature column(s):-\n",
      "['school', 'sex', 'age', 'address', 'famsize', 'Pstatus', 'Medu', 'Fedu', 'Mjob', 'Fjob', 'reason', 'guardian', 'traveltime', 'studytime', 'failures', 'schoolsup', 'famsup', 'paid', 'activities', 'nursery', 'higher', 'internet', 'romantic', 'famrel', 'freetime', 'goout', 'Dalc', 'Walc', 'health', 'absences']\n",
      "Target column: passed\n",
      "\n",
      "Feature values:-\n",
      "  school sex  age address famsize Pstatus  Medu  Fedu     Mjob      Fjob  \\\n",
      "0     GP   F   18       U     GT3       A     4     4  at_home   teacher   \n",
      "1     GP   F   17       U     GT3       T     1     1  at_home     other   \n",
      "2     GP   F   15       U     LE3       T     1     1  at_home     other   \n",
      "3     GP   F   15       U     GT3       T     4     2   health  services   \n",
      "4     GP   F   16       U     GT3       T     3     3    other     other   \n",
      "\n",
      "    ...    higher internet  romantic  famrel  freetime goout Dalc Walc health  \\\n",
      "0   ...       yes       no        no       4         3     4    1    1      3   \n",
      "1   ...       yes      yes        no       5         3     3    1    1      3   \n",
      "2   ...       yes      yes        no       4         3     2    2    3      3   \n",
      "3   ...       yes      yes       yes       3         2     2    1    1      5   \n",
      "4   ...       yes       no        no       4         3     2    1    2      5   \n",
      "\n",
      "  absences  \n",
      "0        6  \n",
      "1        4  \n",
      "2       10  \n",
      "3        2  \n",
      "4        4  \n",
      "\n",
      "[5 rows x 30 columns]\n"
     ]
    }
   ],
   "source": [
    "# Extract feature (X) and target (y) columns\n",
    "feature_cols = list(student_data.columns[:-1])  # all columns but last are features\n",
    "target_col = student_data.columns[-1]  # last column is the target/label\n",
    "print \"Feature column(s):-\\n{}\".format(feature_cols)\n",
    "print \"Target column: {}\".format(target_col)\n",
    "\n",
    "X_all = student_data[feature_cols]  # feature values for all students\n",
    "y_all = student_data[target_col]  # corresponding targets/labels\n",
    "print \"\\nFeature values:-\"\n",
    "print X_all.head()  # print the first 5 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess feature columns\n",
    "\n",
    "As you can see, there are several non-numeric columns that need to be converted! Many of them are simply `yes`/`no`, e.g. `internet`. These can be reasonably converted into `1`/`0` (binary) values.\n",
    "\n",
    "Other columns, like `Mjob` and `Fjob`, have more than two values, and are known as _categorical variables_. The recommended way to handle such a column is to create as many columns as possible values (e.g. `Fjob_teacher`, `Fjob_other`, `Fjob_services`, etc.), and assign a `1` to one of them and `0` to all others.\n",
    "\n",
    "These generated columns are sometimes called _dummy variables_, and we will use the [`pandas.get_dummies()`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html?highlight=get_dummies#pandas.get_dummies) function to perform this transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed feature columns (48):-\n",
      "['school_GP', 'school_MS', 'sex_F', 'sex_M', 'age', 'address_R', 'address_U', 'famsize_GT3', 'famsize_LE3', 'Pstatus_A', 'Pstatus_T', 'Medu', 'Fedu', 'Mjob_at_home', 'Mjob_health', 'Mjob_other', 'Mjob_services', 'Mjob_teacher', 'Fjob_at_home', 'Fjob_health', 'Fjob_other', 'Fjob_services', 'Fjob_teacher', 'reason_course', 'reason_home', 'reason_other', 'reason_reputation', 'guardian_father', 'guardian_mother', 'guardian_other', 'traveltime', 'studytime', 'failures', 'schoolsup', 'famsup', 'paid', 'activities', 'nursery', 'higher', 'internet', 'romantic', 'famrel', 'freetime', 'goout', 'Dalc', 'Walc', 'health', 'absences']\n"
     ]
    }
   ],
   "source": [
    "# Preprocess feature columns\n",
    "def preprocess_features(X):\n",
    "    outX = pd.DataFrame(index=X.index)  # output dataframe, initially empty\n",
    "\n",
    "    # Check each column\n",
    "    for col, col_data in X.iteritems():\n",
    "        # If data type is non-numeric, try to replace all yes/no values with 1/0\n",
    "        if col_data.dtype == object:\n",
    "            col_data = col_data.replace(['yes', 'no'], [1, 0])\n",
    "        # Note: This should change the data type for yes/no columns to int\n",
    "\n",
    "        # If still non-numeric, convert to one or more dummy variables\n",
    "        if col_data.dtype == object:\n",
    "            col_data = pd.get_dummies(col_data, prefix=col)  # e.g. 'school' => 'school_GP', 'school_MS'\n",
    "\n",
    "        outX = outX.join(col_data)  # collect column(s) in output dataframe\n",
    "\n",
    "    return outX\n",
    "\n",
    "X_all = preprocess_features(X_all)\n",
    "print \"Processed feature columns ({}):-\\n{}\".format(len(X_all.columns), list(X_all.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data into training and test sets\n",
    "\n",
    "So far, we have converted all _categorical_ features into numeric values. In this next step, we split the data (both features and corresponding labels) into training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 300 samples\n",
      "Test set: 95 samples\n"
     ]
    }
   ],
   "source": [
    "# First, decide how many training vs test samples you want\n",
    "num_all = student_data.shape[0]  # same as len(student_data)\n",
    "num_train = 300  # about 75% of the data\n",
    "num_test = num_all - num_train\n",
    "\n",
    "# TODO: Then, select features (X) and corresponding labels (y) for the training and test sets\n",
    "# Note: Shuffle the data or randomly select samples to avoid any bias due to ordering in the dataset\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_all, y_all, test_size = .24, random_state = 0)\n",
    "\n",
    "print \"Training set: {} samples\".format(X_train.shape[0])\n",
    "print \"Test set: {} samples\".format(X_test.shape[0])\n",
    "# Note: If you need a validation set, extract it from within training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training and Evaluating Models\n",
    "Choose 3 supervised learning models that are available in scikit-learn, and appropriate for this problem. For each model:\n",
    "\n",
    "- What are the general applications of this model? What are its strengths and weaknesses?\n",
    "- Given what you know about the data so far, why did you choose this model to apply?\n",
    "- Fit this model to the training data, try to predict labels (for both training and test sets), and measure the F<sub>1</sub> score. Repeat this process with different training set sizes (100, 200, 300), keeping test set constant.\n",
    "\n",
    "Produce a table showing training time, prediction time, F<sub>1</sub> score on training set and F<sub>1</sub> score on test set, for each training set size.\n",
    "\n",
    "Note: You need to produce 3 such tables - one for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Helper Functions\n",
    "import time\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Return the classifier's training time\n",
    "def timeTraining(clf, X_train, y_train):\n",
    "    start = time.time()\n",
    "    clf.fit(X_train, y_train)\n",
    "    end = time.time()\n",
    "    return \"{:.3f} s\".format(end - start)\n",
    "\n",
    "# Return the classifier's predictions and prediction time\n",
    "def predictAndTime(clf, features):\n",
    "    start = time.time()\n",
    "    y_pred = clf.predict(features)\n",
    "    end = time.time()\n",
    "    return y_pred, \"{:.3f} s\".format(end - start)\n",
    "\n",
    "# Return the f1 score for the target values and predictions\n",
    "def F1(target, prediction):\n",
    "    return f1_score(target.values, prediction, pos_label='yes')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# To get my bearings I wanted to try most of the classifiers seen in class out of the box.\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "# Array of classifiers\n",
    "clfs = [DecisionTreeClassifier(criterion = \"entropy\"),\n",
    "        SVC(C = 1.0, kernel=\"rbf\"),\n",
    "        GaussianNB(),\n",
    "        AdaBoostClassifier(),\n",
    "        KNeighborsClassifier(n_neighbors = 3)]\n",
    " \n",
    "#Gathering Table column and index labels\n",
    "classifier_names = [clf.__class__.__name__ for clf in clfs]\n",
    "benchmarks = [\"Training time\",  \"F1 score training set\",\"Prediction time\", \"F1 score test set\"]\n",
    "table = pd.DataFrame(columns = classifier_names, index = benchmarks)\n",
    "\n",
    "# Fit Classifiers\n",
    "for clf in clfs:   \n",
    "    classifier   = clf.__class__.__name__\n",
    "    t_train = timeTraining(clf, X_train, y_train)\n",
    "    pred_train_set = predictAndTime(clf, X_train)[0]\n",
    "    pred_test_set, t_test = predictAndTime(clf,X_test)  \n",
    "\n",
    "    table[classifier]['Training time']         = t_train\n",
    "    table[classifier]['F1 score training set'] = F1(y_train, pred_train_set)\n",
    "    table[classifier]['Prediction time']       = t_test\n",
    "    table[classifier]['F1 score test set']     = F1(y_test, pred_test_set)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we manipulated it, the data was strongly categorical. Even age, though ordered,\n",
    "basically could be thought of as buckets between 15 and 22 in this problem. So initially I thought that maybe \n",
    "the DecisionTree would be the most apt to deal with it. Even though we saw in the lectures that decision trees \n",
    "handle continuous data as well, the motivation for the algorithm presented in class, made me think that it would handle this \"categorical\" data well (even though we made it numeric). However after the previous experiments, I don't feel I can conclude any algorithm is particularly suited or ill-suited here. I decided to further explore them with grid search. To get my bearings I wanted to try most of the classifiers seen in class out of the box, excluding neural net since its recommended use requires the features to be scaled. The result of this can be seen in the table below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DecisionTreeClassifier</th>\n",
       "      <th>SVC</th>\n",
       "      <th>GaussianNB</th>\n",
       "      <th>AdaBoostClassifier</th>\n",
       "      <th>KNeighborsClassifier</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Training time</th>\n",
       "      <td>0.010 s</td>\n",
       "      <td>0.023 s</td>\n",
       "      <td>0.002 s</td>\n",
       "      <td>0.158 s</td>\n",
       "      <td>0.001 s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1 score training set</th>\n",
       "      <td>1</td>\n",
       "      <td>0.869198</td>\n",
       "      <td>0.808824</td>\n",
       "      <td>0.868778</td>\n",
       "      <td>0.886878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Prediction time</th>\n",
       "      <td>0.001 s</td>\n",
       "      <td>0.005 s</td>\n",
       "      <td>0.001 s</td>\n",
       "      <td>0.007 s</td>\n",
       "      <td>0.003 s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1 score test set</th>\n",
       "      <td>0.755906</td>\n",
       "      <td>0.758621</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.779412</td>\n",
       "      <td>0.721805</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      DecisionTreeClassifier       SVC GaussianNB  \\\n",
       "Training time                        0.010 s   0.023 s    0.002 s   \n",
       "F1 score training set                      1  0.869198   0.808824   \n",
       "Prediction time                      0.001 s   0.005 s    0.001 s   \n",
       "F1 score test set                   0.755906  0.758621       0.75   \n",
       "\n",
       "                      AdaBoostClassifier KNeighborsClassifier  \n",
       "Training time                    0.158 s              0.001 s  \n",
       "F1 score training set           0.868778             0.886878  \n",
       "Prediction time                  0.007 s              0.003 s  \n",
       "F1 score test set               0.779412             0.721805  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML  \n",
    "display(table)\n",
    "#HTML(table.to_html())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Helper function makeTable\n",
    "def makeTable(clf, training_sizes, X_tr, X_t, y_tr, y_t):\n",
    "    \n",
    "    #Gathering column and row labels for the table\n",
    "    benchmarks = [\"Training time\",  \"F1 score training set\",\"Prediction time\", \"F1 score test set\"]\n",
    "    size_labels = [\"Training samples: {}\".format(s) for s in training_sizes]\n",
    "    table = pd.DataFrame(columns = benchmarks, index = size_labels)\n",
    "    \n",
    "    for i, size in enumerate(training_sizes):\n",
    "        #Use only the first \"size\" number of samples\n",
    "        X_train, X_test, y_train, y_test = [df.iloc[:size] for df in [X_tr, X_t, y_tr, y_t]]\n",
    "        \n",
    "        #Compute benchmarks\n",
    "        t_train    = timeTraining(clf, X_train, y_train)\n",
    "        pred_train_set = predictAndTime(clf, X_train)[0]\n",
    "        pred_test_set, t_test = predictAndTime(clf,X_test)  \n",
    "        \n",
    "        #fill table\n",
    "        table['Training time'][i]    = t_train\n",
    "        table['F1 score training set'][i] = F1(y_train, pred_train_set)\n",
    "        table['Prediction time'][i]  = t_test\n",
    "        table['F1 score test set'][i] = F1(y_test, pred_test_set)\n",
    "        \n",
    "    return table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Training time</th>\n",
       "      <th>F1 score training set</th>\n",
       "      <th>Prediction time</th>\n",
       "      <th>F1 score test set</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Training samples: 50</th>\n",
       "      <td>0.002 s</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001 s</td>\n",
       "      <td>0.80597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Training samples: 100</th>\n",
       "      <td>0.003 s</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000 s</td>\n",
       "      <td>0.725806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Training samples: 150</th>\n",
       "      <td>0.004 s</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000 s</td>\n",
       "      <td>0.697674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Training samples: 200</th>\n",
       "      <td>0.004 s</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000 s</td>\n",
       "      <td>0.71875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Training samples: 250</th>\n",
       "      <td>0.005 s</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000 s</td>\n",
       "      <td>0.710744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Training samples: 300</th>\n",
       "      <td>0.005 s</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000 s</td>\n",
       "      <td>0.765625</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Training time F1 score training set Prediction time  \\\n",
       "Training samples: 50        0.002 s                     1         0.001 s   \n",
       "Training samples: 100       0.003 s                     1         0.000 s   \n",
       "Training samples: 150       0.004 s                     1         0.000 s   \n",
       "Training samples: 200       0.004 s                     1         0.000 s   \n",
       "Training samples: 250       0.005 s                     1         0.000 s   \n",
       "Training samples: 300       0.005 s                     1         0.000 s   \n",
       "\n",
       "                      F1 score test set  \n",
       "Training samples: 50            0.80597  \n",
       "Training samples: 100          0.725806  \n",
       "Training samples: 150          0.697674  \n",
       "Training samples: 200           0.71875  \n",
       "Training samples: 250          0.710744  \n",
       "Training samples: 300          0.765625  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Training time</th>\n",
       "      <th>F1 score training set</th>\n",
       "      <th>Prediction time</th>\n",
       "      <th>F1 score test set</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Training samples: 50</th>\n",
       "      <td>0.001 s</td>\n",
       "      <td>0.90625</td>\n",
       "      <td>0.001 s</td>\n",
       "      <td>0.738462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Training samples: 100</th>\n",
       "      <td>0.002 s</td>\n",
       "      <td>0.85906</td>\n",
       "      <td>0.001 s</td>\n",
       "      <td>0.783784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Training samples: 150</th>\n",
       "      <td>0.003 s</td>\n",
       "      <td>0.870813</td>\n",
       "      <td>0.002 s</td>\n",
       "      <td>0.771429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Training samples: 200</th>\n",
       "      <td>0.004 s</td>\n",
       "      <td>0.869281</td>\n",
       "      <td>0.002 s</td>\n",
       "      <td>0.77551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Training samples: 250</th>\n",
       "      <td>0.006 s</td>\n",
       "      <td>0.879177</td>\n",
       "      <td>0.002 s</td>\n",
       "      <td>0.758621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Training samples: 300</th>\n",
       "      <td>0.008 s</td>\n",
       "      <td>0.869198</td>\n",
       "      <td>0.002 s</td>\n",
       "      <td>0.758621</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Training time F1 score training set Prediction time  \\\n",
       "Training samples: 50        0.001 s               0.90625         0.001 s   \n",
       "Training samples: 100       0.002 s               0.85906         0.001 s   \n",
       "Training samples: 150       0.003 s              0.870813         0.002 s   \n",
       "Training samples: 200       0.004 s              0.869281         0.002 s   \n",
       "Training samples: 250       0.006 s              0.879177         0.002 s   \n",
       "Training samples: 300       0.008 s              0.869198         0.002 s   \n",
       "\n",
       "                      F1 score test set  \n",
       "Training samples: 50           0.738462  \n",
       "Training samples: 100          0.783784  \n",
       "Training samples: 150          0.771429  \n",
       "Training samples: 200           0.77551  \n",
       "Training samples: 250          0.758621  \n",
       "Training samples: 300          0.758621  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GaussianNB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Training time</th>\n",
       "      <th>F1 score training set</th>\n",
       "      <th>Prediction time</th>\n",
       "      <th>F1 score test set</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Training samples: 50</th>\n",
       "      <td>0.001 s</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000 s</td>\n",
       "      <td>0.468085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Training samples: 100</th>\n",
       "      <td>0.001 s</td>\n",
       "      <td>0.854962</td>\n",
       "      <td>0.000 s</td>\n",
       "      <td>0.748092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Training samples: 150</th>\n",
       "      <td>0.001 s</td>\n",
       "      <td>0.808743</td>\n",
       "      <td>0.000 s</td>\n",
       "      <td>0.736842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Training samples: 200</th>\n",
       "      <td>0.001 s</td>\n",
       "      <td>0.832061</td>\n",
       "      <td>0.000 s</td>\n",
       "      <td>0.713178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Training samples: 250</th>\n",
       "      <td>0.001 s</td>\n",
       "      <td>0.817647</td>\n",
       "      <td>0.000 s</td>\n",
       "      <td>0.746269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Training samples: 300</th>\n",
       "      <td>0.001 s</td>\n",
       "      <td>0.808824</td>\n",
       "      <td>0.000 s</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Training time F1 score training set Prediction time  \\\n",
       "Training samples: 50        0.001 s              0.666667         0.000 s   \n",
       "Training samples: 100       0.001 s              0.854962         0.000 s   \n",
       "Training samples: 150       0.001 s              0.808743         0.000 s   \n",
       "Training samples: 200       0.001 s              0.832061         0.000 s   \n",
       "Training samples: 250       0.001 s              0.817647         0.000 s   \n",
       "Training samples: 300       0.001 s              0.808824         0.000 s   \n",
       "\n",
       "                      F1 score test set  \n",
       "Training samples: 50           0.468085  \n",
       "Training samples: 100          0.748092  \n",
       "Training samples: 150          0.736842  \n",
       "Training samples: 200          0.713178  \n",
       "Training samples: 250          0.746269  \n",
       "Training samples: 300              0.75  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoostClassifier\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Training time</th>\n",
       "      <th>F1 score training set</th>\n",
       "      <th>Prediction time</th>\n",
       "      <th>F1 score test set</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Training samples: 50</th>\n",
       "      <td>0.104 s</td>\n",
       "      <td>1</td>\n",
       "      <td>0.006 s</td>\n",
       "      <td>0.645161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Training samples: 100</th>\n",
       "      <td>0.095 s</td>\n",
       "      <td>0.953846</td>\n",
       "      <td>0.006 s</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Training samples: 150</th>\n",
       "      <td>0.098 s</td>\n",
       "      <td>0.912821</td>\n",
       "      <td>0.006 s</td>\n",
       "      <td>0.757576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Training samples: 200</th>\n",
       "      <td>0.100 s</td>\n",
       "      <td>0.882562</td>\n",
       "      <td>0.006 s</td>\n",
       "      <td>0.805755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Training samples: 250</th>\n",
       "      <td>0.102 s</td>\n",
       "      <td>0.886427</td>\n",
       "      <td>0.006 s</td>\n",
       "      <td>0.776978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Training samples: 300</th>\n",
       "      <td>0.105 s</td>\n",
       "      <td>0.868778</td>\n",
       "      <td>0.006 s</td>\n",
       "      <td>0.779412</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Training time F1 score training set Prediction time  \\\n",
       "Training samples: 50        0.104 s                     1         0.006 s   \n",
       "Training samples: 100       0.095 s              0.953846         0.006 s   \n",
       "Training samples: 150       0.098 s              0.912821         0.006 s   \n",
       "Training samples: 200       0.100 s              0.882562         0.006 s   \n",
       "Training samples: 250       0.102 s              0.886427         0.006 s   \n",
       "Training samples: 300       0.105 s              0.868778         0.006 s   \n",
       "\n",
       "                      F1 score test set  \n",
       "Training samples: 50           0.645161  \n",
       "Training samples: 100              0.72  \n",
       "Training samples: 150          0.757576  \n",
       "Training samples: 200          0.805755  \n",
       "Training samples: 250          0.776978  \n",
       "Training samples: 300          0.779412  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Training time</th>\n",
       "      <th>F1 score training set</th>\n",
       "      <th>Prediction time</th>\n",
       "      <th>F1 score test set</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Training samples: 50</th>\n",
       "      <td>0.001 s</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.002 s</td>\n",
       "      <td>0.761905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Training samples: 100</th>\n",
       "      <td>0.001 s</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.002 s</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Training samples: 150</th>\n",
       "      <td>0.001 s</td>\n",
       "      <td>0.816327</td>\n",
       "      <td>0.003 s</td>\n",
       "      <td>0.677419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Training samples: 200</th>\n",
       "      <td>0.001 s</td>\n",
       "      <td>0.86121</td>\n",
       "      <td>0.002 s</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Training samples: 250</th>\n",
       "      <td>0.001 s</td>\n",
       "      <td>0.889503</td>\n",
       "      <td>0.002 s</td>\n",
       "      <td>0.711111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Training samples: 300</th>\n",
       "      <td>0.001 s</td>\n",
       "      <td>0.886878</td>\n",
       "      <td>0.002 s</td>\n",
       "      <td>0.721805</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Training time F1 score training set Prediction time  \\\n",
       "Training samples: 50        0.001 s                   0.8         0.002 s   \n",
       "Training samples: 100       0.001 s              0.823529         0.002 s   \n",
       "Training samples: 150       0.001 s              0.816327         0.003 s   \n",
       "Training samples: 200       0.001 s               0.86121         0.002 s   \n",
       "Training samples: 250       0.001 s              0.889503         0.002 s   \n",
       "Training samples: 300       0.001 s              0.886878         0.002 s   \n",
       "\n",
       "                      F1 score test set  \n",
       "Training samples: 50           0.761905  \n",
       "Training samples: 100          0.666667  \n",
       "Training samples: 150          0.677419  \n",
       "Training samples: 200          0.666667  \n",
       "Training samples: 250          0.711111  \n",
       "Training samples: 300          0.721805  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test Classifiers with increasing data set size\n",
    "training_sizes = [50,100,150,200,250,300]\n",
    "for clf in clfs:\n",
    "    print clf.__class__.__name__\n",
    "    table = makeTable(clf, training_sizes, X_train, X_test, y_train, y_test)\n",
    "    display(table)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The models were relatively stable as training size increased. The SVC seemed the most stable acrosss different \n",
    "training data set sizes. KNN had a strange u-shaped behavior, starting with a \n",
    "highish score decreasing, and then rising again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tuning classifiers: DecissionTree, SVC, AdaBoost, and KNN\n",
    "chosen_clfs = [DecisionTreeClassifier(criterion = \"entropy\"),\n",
    "               AdaBoostClassifier(),\n",
    "               SVC(),\n",
    "               KNeighborsClassifier()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# tree.export_graphviz(clfs[0].fit(X_train,y_train), out_file='tree.dot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Choosing the Best Model\n",
    "\n",
    "- Based on the experiments you performed earlier, in 1-2 paragraphs explain to the board of supervisors what single model you chose as the best model. Which model is generally the most appropriate based on the available data, limited resources, cost, and performance?\n",
    "- In 1-2 paragraphs explain to the board of supervisors in layman's terms how the final model chosen is supposed to work (for example if you chose a Decision Tree or Support Vector Machine, how does it make a prediction).\n",
    "- Fine-tune the model. Use Gridsearch with at least one important parameter tuned and with at least 3 settings. Use the entire training set for this.\n",
    "- What is the model's final F<sub>1</sub> score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier\n",
      "Grid search time: 15.498 s\n",
      "Parameters of tuned model:  {'max_features': 'log2', 'min_samples_split': 3, 'max_depth': 2, 'min_samples_leaf': 8}\n",
      "f1_score and prediction time on X_test, y_test: \n",
      "0.802919708029 0.000 s\n",
      "------------------\n",
      "\n",
      "AdaBoostClassifier\n",
      "Grid search time: 10.084 s\n",
      "Parameters of tuned model:  {'n_estimators': 50, 'learning_rate': 0.6}\n",
      "f1_score and prediction time on X_test, y_test: \n",
      "0.779411764706 0.007 s\n",
      "------------------\n",
      "\n",
      "SVC\n",
      "Grid search time: 45.106 s\n",
      "Parameters of tuned model:  {'kernel': 'rbf', 'C': 150, 'gamma': 0.0001}\n",
      "f1_score and prediction time on X_test, y_test: \n",
      "0.786206896552 0.002 s\n",
      "------------------\n",
      "\n",
      "KNeighborsClassifier\n",
      "Grid search time: 2.923 s\n",
      "Parameters of tuned model:  {'n_neighbors': 30, 'weights': 'uniform', 'p': 2}\n",
      "f1_score and prediction time on X_test, y_test: \n",
      "0.77027027027 0.003 s\n",
      "------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO: Fine-tune your model and report the best F1 score\n",
    "from sklearn import grid_search\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "r = np.arange\n",
    "scorer = make_scorer(F1)\n",
    "\n",
    "tree_param = { 'max_features':[\"sqrt\", \"log2\"], 'max_depth': range(2,11), 'min_samples_split':range(2,9),\n",
    "               'min_samples_leaf':range(1,9) }\n",
    "    \n",
    "ada_param  = {'n_estimators': [50,70,90,110], 'learning_rate':[.6,1,1.3,2]}\n",
    "    \n",
    "svc_param  = [{'C':[100,150,200], 'kernel':['linear']}, \n",
    "              {'C':[100,150,200], 'gamma':[0.01, 0.001, 0.0001], 'kernel':['rbf']},\n",
    "              {'C':[100,150,200], 'degree':[2,3,4,5], 'coef0':[1,10,100],'kernel':['poly']}]\n",
    "\n",
    "neigh_param = {'n_neighbors' : [10,20,25,30,40], 'weights' : ['uniform', 'distance'], 'p':[1,2,3,5,10]}\n",
    "\n",
    "#Perform grid Search\n",
    "def gridIt(clf, params):\n",
    "    grid_clf = grid_search.GridSearchCV(clf, params, scorer)\n",
    "    print clf.__class__.__name__\n",
    "    print \"Grid search time:\", timeTraining(grid_clf, X_train, y_train)\n",
    "    print \"Parameters of tuned model: \", grid_clf.best_params_\n",
    "    y_pred, predict_t = predictAndTime(grid_clf, X_test)\n",
    "    print \"f1_score and prediction time on X_test, y_test: \"\n",
    "    print F1(y_test, y_pred), predict_t\n",
    "    print '------------------\\n'\n",
    "    \n",
    "gridIt(chosen_clfs[0], tree_param)\n",
    "gridIt(chosen_clfs[1], ada_param)\n",
    "gridIt(chosen_clfs[2], svc_param)\n",
    "gridIt(chosen_clfs[3], neigh_param)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Tree F1 for random state: 0 0.808176628012\n",
      "Average KNN F1 for random state: 0 0.787096774194\n",
      "------------------------------------\n",
      "Average Tree F1 for random state: 5 0.779762642075\n",
      "Average KNN F1 for random state: 5 0.786666666667\n",
      "------------------------------------\n",
      "Average Tree F1 for random state: 10 0.791981446416\n",
      "Average KNN F1 for random state: 10 0.810126582278\n",
      "------------------------------------\n",
      "Average Tree F1 for random state: 20 0.794643739379\n",
      "Average KNN F1 for random state: 20 0.802547770701\n",
      "------------------------------------\n",
      "Average Tree F1 for random state: 30 0.793069562794\n",
      "Average KNN F1 for random state: 30 0.782051282051\n",
      "------------------------------------\n",
      "Average Tree F1 for random state: 40 0.805694255864\n",
      "Average KNN F1 for random state: 40 0.815789473684\n",
      "------------------------------------\n"
     ]
    }
   ],
   "source": [
    "##Confirm successfull DecisionTree and KNN parameters aquired from GridSearchCV\n",
    "\n",
    "dTree = DecisionTreeClassifier(max_features = 'log2', min_samples_split= 3, max_depth= 2, min_samples_leaf= 8)\n",
    "nn = KNeighborsClassifier(n_neighbors= 30, weights = 'uniform', p= 2)  \n",
    "\n",
    "rand_states = [0,5,10,20,30,40]\n",
    "\n",
    "def averageF1(clf, X_tr, y_tr, X_t):\n",
    "    sf1 = 0\n",
    "    for i in range(100):\n",
    "        clf.fit(X_tr,y_tr)\n",
    "        y_pred , p_time = predictAndTime(clf, X_t)\n",
    "        sf1 += F1(y_test, y_pred)\n",
    "        \n",
    "    return sf1/100\n",
    "    \n",
    "for s in rand_states:\n",
    "    X_tr, X_t, y_tr, y_t = train_test_split(X_all, y_all, \n",
    "                                                        test_size = .24, random_state = s)\n",
    "    print \"Average Tree F1 for random state: \" + str(s) + \" \" + str(averageF1(dTree, X_tr, y_tr, X_t))\n",
    "    print \"Average KNN F1 for random state: \"  + str(s) + \" \" + str(averageF1(nn, X_tr, y_tr, X_t))\n",
    "    print \"------------------------------------\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Conclusions\n",
    "**Algorithm Selection:**  \n",
    "* * *\n",
    "My code reviewer pointed out that I had used the whole training set while I was still in model selection phase.\n",
    "After correcting this, the results from grid search in cell [100] became more comprehensible. Based on the computations performed there, although all algorithms (even if only marginally) benefited from the tuning process, two of them in my opinion are good candidates for consideration for final selection. These are DecisionTreeClassifier and KNeighborsClassifier. The SVM was very costly to tune with grid search, taking 45 seconds, with the next longest tuning being DecisionTree's at 15.5 seconds. Adaboost had a relatively fast training time of 10 s, but grid search failed to find a combination of parameters that significantly improved its final f1 score. For the the full training set size of 300, the DecisionTree had a marked f1 score improvement from 0.766 (cell [97]'s output) to 0.803. KNeighborsClassifier similarly benefitted from the grid search to improve its f1 score from 0.722 to 0.770. In previous attempts, when I found a good combination of parameters, performance seemed dependent on the particular run I had just observed. As hinted to me after the code review, the train/test spliting proccess can have marked effects on the algorithm's final performance, especially on small data sets like this one. To check that I had found consistently good parameters with grid search, I did a final test by averaging 100 f1_scores for each of 6 random_state seed values in train_test_split. As can be seen from the output of cell [115], both KNeighbors and DecisionTree sustained the good performance exhibited after grid search in cell [100], and also differed little from one another in f1_score. However KNeighbors grid_search training time was a mere 2.9 seconds. Therefore under the rubric of: available data, limited resources, cost, and performance, KNeighborsClassifier is the best model for use in this student intervention system. \n",
    "\n",
    "**Layman Explanation: KNeighborsClassifier**  \n",
    "* * *\n",
    "The mechanism with which our model decides whether a current student will pass or fail is very intuitive. Each student has 30 attributes associated with him/her. These range from basic descriptors like their age, sex, and health, to behavioral descriptors like whether they are in a romantic relationship, how much time they devote to study, if they have any extra curricular activities. Some of the attributes are of things out of there control like whether they have internet access, the size of their family, and what neighborhood they live in. What are model process does, is that it assigns a relevant number to every one of these attributes. Just like for example you can take a house and assign it a longitude, a latitude, and maybe if it sits on a hill an altitude. In the same way that information like longitude, latitude and altitude, alows us to decide how far away two houses are from each other, we can decide how \"far away\" two particular students are from each other. Based on all those attributes like free time and age and so forth. Since we have information about which students have failed and which have passed, our model basically answers the question; how \"close\" is this student to other students who have passed. Maybe he is \"closer\" to students who fail. We can choose to  compare him/her to the closest _single_ student to determine how likely he is to pass or fail. Usually, however we tune the model to find a _group_ of students closest to him/her, maybe the closest 4 students, or maybe 10 closest students. The exact number is determined while tuning. Since we use students that we know either passed or failed in this group, we can determine if our student in question is closer to others who pass or those who fail. \n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Final F1 score**\n",
    "* * *\n",
    "Given the way I selected the KNeighborsClassifier, I present the average and standard deviation of f1 scores across\n",
    "the different train/test splits of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average f1 score: 0.797379758263\n",
      "Std for the f1 scores 0.0128035136636\n"
     ]
    }
   ],
   "source": [
    "knn_f1_scores = [0.787096774194, 0.786666666667, 0.810126582278, 0.802547770701, 0.782051282051, 0.815789473684]\n",
    "print \"Average f1 score: {}\".format(np.mean(knn_f1_scores))\n",
    "print \"Std for the f1 scores {}\".format(np.std(knn_f1_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
